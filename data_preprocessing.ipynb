{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0fee49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768df02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boilerplate code to connect to mysql server\n",
    "username = 'xqi'\n",
    "password = 'xqi_1234!'\n",
    "host = '52.6.148.1'       \n",
    "port = 3306              \n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5992dc5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1836026195': 'INDIAN RIVER INLET Lvl',\n",
       " '1836026194': 'INDIAN RIVER AT ROSEDALE BEACH Lvl',\n",
       " '271420501': 'LEWES Lvl',\n",
       " '1836026192': 'UNNAMED DITCH ON FRED HUDSON ROAD Lvl',\n",
       " '1836026191': 'LITTLE ASSAWOMAN BAY Lvl',\n",
       " '1836026193': 'VINES CREEK Lvl',\n",
       " '1413617665': 'REHOBOTH BAY AT DEWEY BEACH, DE Lvl'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load station dict from data/station_dict.json as a dictionary\n",
    "with open('data/station_dict.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf73cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['1836026195', '1836026194', '271420501', '1836026192', '1836026191', '1836026193', '1413617665'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()  # should be a list of sensor ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06935f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning function defined!\n"
     ]
    }
   ],
   "source": [
    "def clean_hydro_data(df):\n",
    "    \"\"\"\n",
    "    Clean hydro data by:\n",
    "    1. Setting seconds to 0 for all collect_time values\n",
    "    2. Removing duplicate timestamps\n",
    "    3. Sorting by time\n",
    "    4. Basic data validation\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'collect_time' column\n",
    "    \n",
    "    Returns:\n",
    "        cleaned DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"Original data shape: {df.shape}\")\n",
    "    print(f\"Original time range: {df['collect_time'].min()} to {df['collect_time'].max()}\")\n",
    "    \n",
    "    # Make a copy to avoid modifying original data\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    # Convert collect_time to datetime if it's not already\n",
    "    if df_cleaned['collect_time'].dtype == 'object':\n",
    "        df_cleaned['collect_time'] = pd.to_datetime(df_cleaned['collect_time'])\n",
    "    \n",
    "    # Step 1: Set seconds to 0 (normalize to minute precision)\n",
    "    print(\"\\nStep 1: Normalizing collect_time to minute precision (setting seconds to 0)\")\n",
    "    original_times = df_cleaned['collect_time'].head(5)\n",
    "    print(f\"Original times (first 5): {list(original_times)}\")\n",
    "    \n",
    "    df_cleaned['collect_time'] = df_cleaned['collect_time'].dt.floor('min')\n",
    "    \n",
    "    normalized_times = df_cleaned['collect_time'].head(5)\n",
    "    print(f\"Normalized times (first 5): {list(normalized_times)}\")\n",
    "    \n",
    "    # Step 2: Check for and remove duplicates\n",
    "    print(f\"\\nStep 2: Checking for duplicate timestamps\")\n",
    "    duplicates_before = df_cleaned.duplicated(subset=['collect_time']).sum()\n",
    "    print(f\"Found {duplicates_before} duplicate timestamps\")\n",
    "    \n",
    "    if duplicates_before > 0:\n",
    "        # Show some examples of duplicates\n",
    "        duplicate_times = df_cleaned[df_cleaned.duplicated(subset=['collect_time'], keep=False)]['collect_time'].value_counts().head(3)\n",
    "        print(f\"Most common duplicate times (top 3):\")\n",
    "        for time, count in duplicate_times.items():\n",
    "            print(f\"  {time}: {count} occurrences\")\n",
    "            \n",
    "        # Remove duplicates (keep first occurrence)\n",
    "        df_cleaned = df_cleaned.drop_duplicates(subset=['collect_time'], keep='first')\n",
    "        print(f\"Removed {duplicates_before} duplicate timestamps\")\n",
    "    \n",
    "    # Step 3: Sort by time\n",
    "    print(f\"\\nStep 3: Sorting by collect_time\")\n",
    "    df_cleaned = df_cleaned.sort_values('collect_time').reset_index(drop=True)\n",
    "    \n",
    "    # Step 4: Basic validation and statistics\n",
    "    print(f\"\\nStep 4: Data validation and statistics\")\n",
    "    print(f\"Cleaned data shape: {df_cleaned.shape}\")\n",
    "    print(f\"Cleaned time range: {df_cleaned['collect_time'].min()} to {df_cleaned['collect_time'].max()}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df_cleaned.isnull().sum()\n",
    "    print(f\"Missing values per column:\")\n",
    "    for col, missing in missing_values.items():\n",
    "        if missing > 0:\n",
    "            print(f\"  {col}: {missing} ({missing/len(df_cleaned)*100:.2f}%)\")\n",
    "    \n",
    "    # Time series continuity check\n",
    "    time_diffs = df_cleaned['collect_time'].diff()\n",
    "    most_common_interval = time_diffs.mode().iloc[0] if len(time_diffs.mode()) > 0 else None\n",
    "    print(f\"Most common time interval: {most_common_interval}\")\n",
    "    \n",
    "    # Check for large gaps\n",
    "    large_gaps = time_diffs[time_diffs > pd.Timedelta(hours=1)]\n",
    "    if len(large_gaps) > 0:\n",
    "        print(f\"Found {len(large_gaps)} time gaps larger than 1 hour\")\n",
    "        print(f\"Largest gap: {large_gaps.max()}\")\n",
    "    \n",
    "    print(f\"\\nData cleaning completed!\")\n",
    "    print(f\"Reduction: {len(df) - len(df_cleaned)} rows removed ({(len(df) - len(df_cleaned))/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "print(\"Data cleaning function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "985ce088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor ID: 1836026195, Station Name: INDIAN RIVER INLET Lvl\n",
      "Original data shape: (98642, 6)\n",
      "Original time range: 2024-07-01 00:02:15 to 2025-06-29 23:57:18\n",
      "\n",
      "Step 1: Normalizing collect_time to minute precision (setting seconds to 0)\n",
      "Original times (first 5): [Timestamp('2024-07-01 00:02:15'), Timestamp('2024-07-01 00:02:15'), Timestamp('2024-07-01 00:08:15'), Timestamp('2024-07-01 00:14:15'), Timestamp('2024-07-01 00:20:15')]\n",
      "Normalized times (first 5): [Timestamp('2024-07-01 00:02:00'), Timestamp('2024-07-01 00:02:00'), Timestamp('2024-07-01 00:08:00'), Timestamp('2024-07-01 00:14:00'), Timestamp('2024-07-01 00:20:00')]\n",
      "\n",
      "Step 2: Checking for duplicate timestamps\n",
      "Found 16612 duplicate timestamps\n",
      "Most common duplicate times (top 3):\n",
      "  2025-05-01 16:56:00: 7 occurrences\n",
      "  2024-12-16 16:17:00: 4 occurrences\n",
      "  2024-12-16 14:47:00: 4 occurrences\n",
      "Removed 16612 duplicate timestamps\n",
      "\n",
      "Step 3: Sorting by collect_time\n",
      "\n",
      "Step 4: Data validation and statistics\n",
      "Cleaned data shape: (82030, 6)\n",
      "Cleaned time range: 2024-07-01 00:02:00 to 2025-06-29 23:57:00\n",
      "Missing values per column:\n",
      "Most common time interval: 0 days 00:06:00\n",
      "Found 12 time gaps larger than 1 hour\n",
      "Largest gap: 11 days 00:06:00\n",
      "\n",
      "Data cleaning completed!\n",
      "Reduction: 16612 rows removed (16.84%)\n",
      "Sensor ID: 1836026194, Station Name: INDIAN RIVER AT ROSEDALE BEACH Lvl\n",
      "Original data shape: (98642, 6)\n",
      "Original time range: 2024-07-01 00:02:15 to 2025-06-29 23:57:18\n",
      "\n",
      "Step 1: Normalizing collect_time to minute precision (setting seconds to 0)\n",
      "Original times (first 5): [Timestamp('2024-07-01 00:02:15'), Timestamp('2024-07-01 00:02:15'), Timestamp('2024-07-01 00:08:15'), Timestamp('2024-07-01 00:14:15'), Timestamp('2024-07-01 00:20:15')]\n",
      "Normalized times (first 5): [Timestamp('2024-07-01 00:02:00'), Timestamp('2024-07-01 00:02:00'), Timestamp('2024-07-01 00:08:00'), Timestamp('2024-07-01 00:14:00'), Timestamp('2024-07-01 00:20:00')]\n",
      "\n",
      "Step 2: Checking for duplicate timestamps\n",
      "Found 16612 duplicate timestamps\n",
      "Most common duplicate times (top 3):\n",
      "  2025-05-01 16:56:00: 7 occurrences\n",
      "  2024-12-16 16:17:00: 4 occurrences\n",
      "  2024-12-16 14:47:00: 4 occurrences\n",
      "Removed 16612 duplicate timestamps\n",
      "\n",
      "Step 3: Sorting by collect_time\n",
      "\n",
      "Step 4: Data validation and statistics\n",
      "Cleaned data shape: (82030, 6)\n",
      "Cleaned time range: 2024-07-01 00:02:00 to 2025-06-29 23:57:00\n",
      "Missing values per column:\n",
      "Most common time interval: 0 days 00:06:00\n",
      "Found 12 time gaps larger than 1 hour\n",
      "Largest gap: 11 days 00:06:00\n",
      "\n",
      "Data cleaning completed!\n",
      "Reduction: 16612 rows removed (16.84%)\n",
      "Sensor ID: 1836026194, Station Name: INDIAN RIVER AT ROSEDALE BEACH Lvl\n",
      "Original data shape: (98642, 6)\n",
      "Original time range: 2024-07-01 00:00:20 to 2025-06-29 23:57:45\n",
      "\n",
      "Step 1: Normalizing collect_time to minute precision (setting seconds to 0)\n",
      "Original times (first 5): [Timestamp('2024-07-01 00:00:20'), Timestamp('2024-07-01 00:06:20'), Timestamp('2024-07-01 00:12:20'), Timestamp('2024-07-01 00:12:20'), Timestamp('2024-07-01 00:18:20')]\n",
      "Normalized times (first 5): [Timestamp('2024-07-01 00:00:00'), Timestamp('2024-07-01 00:06:00'), Timestamp('2024-07-01 00:12:00'), Timestamp('2024-07-01 00:12:00'), Timestamp('2024-07-01 00:18:00')]\n",
      "\n",
      "Step 2: Checking for duplicate timestamps\n",
      "Found 16293 duplicate timestamps\n",
      "Most common duplicate times (top 3):\n",
      "  2025-05-01 16:57:00: 7 occurrences\n",
      "  2025-06-12 01:15:00: 5 occurrences\n",
      "  2024-12-16 15:12:00: 4 occurrences\n",
      "Removed 16293 duplicate timestamps\n",
      "\n",
      "Step 3: Sorting by collect_time\n",
      "\n",
      "Step 4: Data validation and statistics\n",
      "Cleaned data shape: (82349, 6)\n",
      "Cleaned time range: 2024-07-01 00:00:00 to 2025-06-29 23:57:00\n",
      "Missing values per column:\n",
      "Most common time interval: 0 days 00:06:00\n",
      "Found 13 time gaps larger than 1 hour\n",
      "Largest gap: 11 days 00:12:00\n",
      "\n",
      "Data cleaning completed!\n",
      "Reduction: 16293 rows removed (16.52%)\n",
      "Sensor ID: 271420501, Station Name: LEWES Lvl\n",
      "Original data shape: (98642, 6)\n",
      "Original time range: 2024-07-01 00:00:20 to 2025-06-29 23:57:45\n",
      "\n",
      "Step 1: Normalizing collect_time to minute precision (setting seconds to 0)\n",
      "Original times (first 5): [Timestamp('2024-07-01 00:00:20'), Timestamp('2024-07-01 00:06:20'), Timestamp('2024-07-01 00:12:20'), Timestamp('2024-07-01 00:12:20'), Timestamp('2024-07-01 00:18:20')]\n",
      "Normalized times (first 5): [Timestamp('2024-07-01 00:00:00'), Timestamp('2024-07-01 00:06:00'), Timestamp('2024-07-01 00:12:00'), Timestamp('2024-07-01 00:12:00'), Timestamp('2024-07-01 00:18:00')]\n",
      "\n",
      "Step 2: Checking for duplicate timestamps\n",
      "Found 16293 duplicate timestamps\n",
      "Most common duplicate times (top 3):\n",
      "  2025-05-01 16:57:00: 7 occurrences\n",
      "  2025-06-12 01:15:00: 5 occurrences\n",
      "  2024-12-16 15:12:00: 4 occurrences\n",
      "Removed 16293 duplicate timestamps\n",
      "\n",
      "Step 3: Sorting by collect_time\n",
      "\n",
      "Step 4: Data validation and statistics\n",
      "Cleaned data shape: (82349, 6)\n",
      "Cleaned time range: 2024-07-01 00:00:00 to 2025-06-29 23:57:00\n",
      "Missing values per column:\n",
      "Most common time interval: 0 days 00:06:00\n",
      "Found 13 time gaps larger than 1 hour\n",
      "Largest gap: 11 days 00:12:00\n",
      "\n",
      "Data cleaning completed!\n",
      "Reduction: 16293 rows removed (16.52%)\n",
      "Sensor ID: 271420501, Station Name: LEWES Lvl\n",
      "Original data shape: (98639, 6)\n",
      "Original time range: 2024-07-01 00:06:00 to 2025-06-29 23:54:00\n",
      "\n",
      "Step 1: Normalizing collect_time to minute precision (setting seconds to 0)\n",
      "Original times (first 5): [Timestamp('2024-07-01 00:06:00'), Timestamp('2024-07-01 00:12:00'), Timestamp('2024-07-01 00:12:00'), Timestamp('2024-07-01 00:18:00'), Timestamp('2024-07-01 00:30:00')]\n",
      "Normalized times (first 5): [Timestamp('2024-07-01 00:06:00'), Timestamp('2024-07-01 00:12:00'), Timestamp('2024-07-01 00:12:00'), Timestamp('2024-07-01 00:18:00'), Timestamp('2024-07-01 00:30:00')]\n",
      "\n",
      "Step 2: Checking for duplicate timestamps\n",
      "Found 27946 duplicate timestamps\n",
      "Most common duplicate times (top 3):\n",
      "  2025-04-25 10:30:00: 74 occurrences\n",
      "  2025-04-24 13:36:00: 24 occurrences\n",
      "  2025-01-09 16:24:00: 19 occurrences\n",
      "Removed 27946 duplicate timestamps\n",
      "\n",
      "Step 3: Sorting by collect_time\n",
      "\n",
      "Step 4: Data validation and statistics\n",
      "Cleaned data shape: (70693, 6)\n",
      "Cleaned time range: 2024-07-01 00:06:00 to 2025-06-29 23:54:00\n",
      "Missing values per column:\n",
      "Most common time interval: 0 days 00:06:00\n",
      "Found 16 time gaps larger than 1 hour\n",
      "Largest gap: 11 days 00:18:00\n",
      "\n",
      "Data cleaning completed!\n",
      "Reduction: 27946 rows removed (28.33%)\n",
      "Sensor ID: 1836026192, Station Name: UNNAMED DITCH ON FRED HUDSON ROAD Lvl\n",
      "Original data shape: (98639, 6)\n",
      "Original time range: 2024-07-01 00:06:00 to 2025-06-29 23:54:00\n",
      "\n",
      "Step 1: Normalizing collect_time to minute precision (setting seconds to 0)\n",
      "Original times (first 5): [Timestamp('2024-07-01 00:06:00'), Timestamp('2024-07-01 00:12:00'), Timestamp('2024-07-01 00:12:00'), Timestamp('2024-07-01 00:18:00'), Timestamp('2024-07-01 00:30:00')]\n",
      "Normalized times (first 5): [Timestamp('2024-07-01 00:06:00'), Timestamp('2024-07-01 00:12:00'), Timestamp('2024-07-01 00:12:00'), Timestamp('2024-07-01 00:18:00'), Timestamp('2024-07-01 00:30:00')]\n",
      "\n",
      "Step 2: Checking for duplicate timestamps\n",
      "Found 27946 duplicate timestamps\n",
      "Most common duplicate times (top 3):\n",
      "  2025-04-25 10:30:00: 74 occurrences\n",
      "  2025-04-24 13:36:00: 24 occurrences\n",
      "  2025-01-09 16:24:00: 19 occurrences\n",
      "Removed 27946 duplicate timestamps\n",
      "\n",
      "Step 3: Sorting by collect_time\n",
      "\n",
      "Step 4: Data validation and statistics\n",
      "Cleaned data shape: (70693, 6)\n",
      "Cleaned time range: 2024-07-01 00:06:00 to 2025-06-29 23:54:00\n",
      "Missing values per column:\n",
      "Most common time interval: 0 days 00:06:00\n",
      "Found 16 time gaps larger than 1 hour\n",
      "Largest gap: 11 days 00:18:00\n",
      "\n",
      "Data cleaning completed!\n",
      "Reduction: 27946 rows removed (28.33%)\n",
      "Sensor ID: 1836026192, Station Name: UNNAMED DITCH ON FRED HUDSON ROAD Lvl\n",
      "Original data shape: (98642, 6)\n",
      "Original time range: 2024-07-01 00:00:48 to 2025-06-29 23:59:25\n",
      "\n",
      "Step 1: Normalizing collect_time to minute precision (setting seconds to 0)\n",
      "Original times (first 5): [Timestamp('2024-07-01 00:00:48'), Timestamp('2024-07-01 00:06:48'), Timestamp('2024-07-01 00:06:48'), Timestamp('2024-07-01 00:12:48'), Timestamp('2024-07-01 00:18:48')]\n",
      "Normalized times (first 5): [Timestamp('2024-07-01 00:00:00'), Timestamp('2024-07-01 00:06:00'), Timestamp('2024-07-01 00:06:00'), Timestamp('2024-07-01 00:12:00'), Timestamp('2024-07-01 00:18:00')]\n",
      "\n",
      "Step 2: Checking for duplicate timestamps\n",
      "Found 16720 duplicate timestamps\n",
      "Most common duplicate times (top 3):\n",
      "  2025-03-18 02:38:00: 63 occurrences\n",
      "  2025-05-01 16:53:00: 7 occurrences\n",
      "  2024-12-16 16:07:00: 4 occurrences\n",
      "Removed 16720 duplicate timestamps\n",
      "\n",
      "Step 3: Sorting by collect_time\n",
      "\n",
      "Step 4: Data validation and statistics\n",
      "Cleaned data shape: (81922, 6)\n",
      "Cleaned time range: 2024-07-01 00:00:00 to 2025-06-29 23:59:00\n",
      "Missing values per column:\n",
      "Most common time interval: 0 days 00:06:00\n",
      "Found 14 time gaps larger than 1 hour\n",
      "Largest gap: 11 days 00:12:00\n",
      "\n",
      "Data cleaning completed!\n",
      "Reduction: 16720 rows removed (16.95%)\n",
      "Sensor ID: 1836026191, Station Name: LITTLE ASSAWOMAN BAY Lvl\n",
      "Original data shape: (98642, 6)\n",
      "Original time range: 2024-07-01 00:00:48 to 2025-06-29 23:59:25\n",
      "\n",
      "Step 1: Normalizing collect_time to minute precision (setting seconds to 0)\n",
      "Original times (first 5): [Timestamp('2024-07-01 00:00:48'), Timestamp('2024-07-01 00:06:48'), Timestamp('2024-07-01 00:06:48'), Timestamp('2024-07-01 00:12:48'), Timestamp('2024-07-01 00:18:48')]\n",
      "Normalized times (first 5): [Timestamp('2024-07-01 00:00:00'), Timestamp('2024-07-01 00:06:00'), Timestamp('2024-07-01 00:06:00'), Timestamp('2024-07-01 00:12:00'), Timestamp('2024-07-01 00:18:00')]\n",
      "\n",
      "Step 2: Checking for duplicate timestamps\n",
      "Found 16720 duplicate timestamps\n",
      "Most common duplicate times (top 3):\n",
      "  2025-03-18 02:38:00: 63 occurrences\n",
      "  2025-05-01 16:53:00: 7 occurrences\n",
      "  2024-12-16 16:07:00: 4 occurrences\n",
      "Removed 16720 duplicate timestamps\n",
      "\n",
      "Step 3: Sorting by collect_time\n",
      "\n",
      "Step 4: Data validation and statistics\n",
      "Cleaned data shape: (81922, 6)\n",
      "Cleaned time range: 2024-07-01 00:00:00 to 2025-06-29 23:59:00\n",
      "Missing values per column:\n",
      "Most common time interval: 0 days 00:06:00\n",
      "Found 14 time gaps larger than 1 hour\n",
      "Largest gap: 11 days 00:12:00\n",
      "\n",
      "Data cleaning completed!\n",
      "Reduction: 16720 rows removed (16.95%)\n",
      "Sensor ID: 1836026191, Station Name: LITTLE ASSAWOMAN BAY Lvl\n",
      "Original data shape: (98640, 6)\n",
      "Original time range: 2024-07-01 00:03:47 to 2025-06-29 23:55:04\n",
      "\n",
      "Step 1: Normalizing collect_time to minute precision (setting seconds to 0)\n",
      "Original times (first 5): [Timestamp('2024-07-01 00:03:47'), Timestamp('2024-07-01 00:09:47'), Timestamp('2024-07-01 00:15:47'), Timestamp('2024-07-01 00:21:47'), Timestamp('2024-07-01 00:21:47')]\n",
      "Normalized times (first 5): [Timestamp('2024-07-01 00:03:00'), Timestamp('2024-07-01 00:09:00'), Timestamp('2024-07-01 00:15:00'), Timestamp('2024-07-01 00:21:00'), Timestamp('2024-07-01 00:21:00')]\n",
      "\n",
      "Step 2: Checking for duplicate timestamps\n",
      "Found 16488 duplicate timestamps\n",
      "Most common duplicate times (top 3):\n",
      "  2025-05-01 16:54:00: 7 occurrences\n",
      "  2024-12-16 14:17:00: 4 occurrences\n",
      "  2024-12-16 14:47:00: 4 occurrences\n",
      "Removed 16488 duplicate timestamps\n",
      "\n",
      "Step 3: Sorting by collect_time\n",
      "\n",
      "Step 4: Data validation and statistics\n",
      "Cleaned data shape: (82152, 6)\n",
      "Cleaned time range: 2024-07-01 00:03:00 to 2025-06-29 23:55:00\n",
      "Missing values per column:\n",
      "Most common time interval: 0 days 00:06:00\n",
      "Found 12 time gaps larger than 1 hour\n",
      "Largest gap: 11 days 00:06:00\n",
      "\n",
      "Data cleaning completed!\n",
      "Reduction: 16488 rows removed (16.72%)\n",
      "Sensor ID: 1836026193, Station Name: VINES CREEK Lvl\n",
      "Original data shape: (98640, 6)\n",
      "Original time range: 2024-07-01 00:03:47 to 2025-06-29 23:55:04\n",
      "\n",
      "Step 1: Normalizing collect_time to minute precision (setting seconds to 0)\n",
      "Original times (first 5): [Timestamp('2024-07-01 00:03:47'), Timestamp('2024-07-01 00:09:47'), Timestamp('2024-07-01 00:15:47'), Timestamp('2024-07-01 00:21:47'), Timestamp('2024-07-01 00:21:47')]\n",
      "Normalized times (first 5): [Timestamp('2024-07-01 00:03:00'), Timestamp('2024-07-01 00:09:00'), Timestamp('2024-07-01 00:15:00'), Timestamp('2024-07-01 00:21:00'), Timestamp('2024-07-01 00:21:00')]\n",
      "\n",
      "Step 2: Checking for duplicate timestamps\n",
      "Found 16488 duplicate timestamps\n",
      "Most common duplicate times (top 3):\n",
      "  2025-05-01 16:54:00: 7 occurrences\n",
      "  2024-12-16 14:17:00: 4 occurrences\n",
      "  2024-12-16 14:47:00: 4 occurrences\n",
      "Removed 16488 duplicate timestamps\n",
      "\n",
      "Step 3: Sorting by collect_time\n",
      "\n",
      "Step 4: Data validation and statistics\n",
      "Cleaned data shape: (82152, 6)\n",
      "Cleaned time range: 2024-07-01 00:03:00 to 2025-06-29 23:55:00\n",
      "Missing values per column:\n",
      "Most common time interval: 0 days 00:06:00\n",
      "Found 12 time gaps larger than 1 hour\n",
      "Largest gap: 11 days 00:06:00\n",
      "\n",
      "Data cleaning completed!\n",
      "Reduction: 16488 rows removed (16.72%)\n",
      "Sensor ID: 1836026193, Station Name: VINES CREEK Lvl\n",
      "Original data shape: (98640, 6)\n",
      "Original time range: 2024-07-01 00:06:27 to 2025-06-29 23:49:49\n",
      "\n",
      "Step 1: Normalizing collect_time to minute precision (setting seconds to 0)\n",
      "Original times (first 5): [Timestamp('2024-07-01 00:06:27'), Timestamp('2024-07-01 00:10:38'), Timestamp('2024-07-01 00:10:38'), Timestamp('2024-07-01 00:18:27'), Timestamp('2024-07-01 00:25:38')]\n",
      "Normalized times (first 5): [Timestamp('2024-07-01 00:06:00'), Timestamp('2024-07-01 00:10:00'), Timestamp('2024-07-01 00:10:00'), Timestamp('2024-07-01 00:18:00'), Timestamp('2024-07-01 00:25:00')]\n",
      "\n",
      "Step 2: Checking for duplicate timestamps\n",
      "Found 51028 duplicate timestamps\n",
      "Most common duplicate times (top 3):\n",
      "  2024-07-03 11:10:00: 30 occurrences\n",
      "  2024-07-28 02:06:00: 16 occurrences\n",
      "  2024-07-28 07:11:00: 15 occurrences\n",
      "Removed 51028 duplicate timestamps\n",
      "\n",
      "Step 3: Sorting by collect_time\n",
      "\n",
      "Step 4: Data validation and statistics\n",
      "Cleaned data shape: (47612, 6)\n",
      "Cleaned time range: 2024-07-01 00:06:00 to 2025-06-29 23:49:00\n",
      "Missing values per column:\n",
      "Most common time interval: 0 days 00:15:00\n",
      "Found 17 time gaps larger than 1 hour\n",
      "Largest gap: 11 days 00:01:00\n",
      "\n",
      "Data cleaning completed!\n",
      "Reduction: 51028 rows removed (51.73%)\n",
      "Sensor ID: 1413617665, Station Name: REHOBOTH BAY AT DEWEY BEACH, DE Lvl\n",
      "Original data shape: (98640, 6)\n",
      "Original time range: 2024-07-01 00:06:27 to 2025-06-29 23:49:49\n",
      "\n",
      "Step 1: Normalizing collect_time to minute precision (setting seconds to 0)\n",
      "Original times (first 5): [Timestamp('2024-07-01 00:06:27'), Timestamp('2024-07-01 00:10:38'), Timestamp('2024-07-01 00:10:38'), Timestamp('2024-07-01 00:18:27'), Timestamp('2024-07-01 00:25:38')]\n",
      "Normalized times (first 5): [Timestamp('2024-07-01 00:06:00'), Timestamp('2024-07-01 00:10:00'), Timestamp('2024-07-01 00:10:00'), Timestamp('2024-07-01 00:18:00'), Timestamp('2024-07-01 00:25:00')]\n",
      "\n",
      "Step 2: Checking for duplicate timestamps\n",
      "Found 51028 duplicate timestamps\n",
      "Most common duplicate times (top 3):\n",
      "  2024-07-03 11:10:00: 30 occurrences\n",
      "  2024-07-28 02:06:00: 16 occurrences\n",
      "  2024-07-28 07:11:00: 15 occurrences\n",
      "Removed 51028 duplicate timestamps\n",
      "\n",
      "Step 3: Sorting by collect_time\n",
      "\n",
      "Step 4: Data validation and statistics\n",
      "Cleaned data shape: (47612, 6)\n",
      "Cleaned time range: 2024-07-01 00:06:00 to 2025-06-29 23:49:00\n",
      "Missing values per column:\n",
      "Most common time interval: 0 days 00:15:00\n",
      "Found 17 time gaps larger than 1 hour\n",
      "Largest gap: 11 days 00:01:00\n",
      "\n",
      "Data cleaning completed!\n",
      "Reduction: 51028 rows removed (51.73%)\n",
      "Sensor ID: 1413617665, Station Name: REHOBOTH BAY AT DEWEY BEACH, DE Lvl\n",
      "Original data shape: (27148, 6)\n",
      "Original time range: 2025-03-26 13:36:00 to 2025-06-29 23:36:00\n",
      "\n",
      "Step 1: Normalizing collect_time to minute precision (setting seconds to 0)\n",
      "Original times (first 5): [Timestamp('2025-03-26 13:36:00'), Timestamp('2025-03-26 13:36:00'), Timestamp('2025-03-26 13:36:00'), Timestamp('2025-03-26 13:36:00'), Timestamp('2025-03-26 13:36:00')]\n",
      "Normalized times (first 5): [Timestamp('2025-03-26 13:36:00'), Timestamp('2025-03-26 13:36:00'), Timestamp('2025-03-26 13:36:00'), Timestamp('2025-03-26 13:36:00'), Timestamp('2025-03-26 13:36:00')]\n",
      "\n",
      "Step 2: Checking for duplicate timestamps\n",
      "Found 23818 duplicate timestamps\n",
      "Most common duplicate times (top 3):\n",
      "  2025-06-15 06:36:00: 36 occurrences\n",
      "  2025-06-12 08:42:00: 35 occurrences\n",
      "  2025-06-06 14:36:00: 24 occurrences\n",
      "Removed 23818 duplicate timestamps\n",
      "\n",
      "Step 3: Sorting by collect_time\n",
      "\n",
      "Step 4: Data validation and statistics\n",
      "Cleaned data shape: (3330, 6)\n",
      "Cleaned time range: 2025-03-26 13:36:00 to 2025-06-29 23:36:00\n",
      "Missing values per column:\n",
      "Most common time interval: 0 days 01:00:00\n",
      "Found 31 time gaps larger than 1 hour\n",
      "Largest gap: 0 days 07:00:00\n",
      "\n",
      "Data cleaning completed!\n",
      "Reduction: 23818 rows removed (87.73%)\n",
      "Original data shape: (27148, 6)\n",
      "Original time range: 2025-03-26 13:36:00 to 2025-06-29 23:36:00\n",
      "\n",
      "Step 1: Normalizing collect_time to minute precision (setting seconds to 0)\n",
      "Original times (first 5): [Timestamp('2025-03-26 13:36:00'), Timestamp('2025-03-26 13:36:00'), Timestamp('2025-03-26 13:36:00'), Timestamp('2025-03-26 13:36:00'), Timestamp('2025-03-26 13:36:00')]\n",
      "Normalized times (first 5): [Timestamp('2025-03-26 13:36:00'), Timestamp('2025-03-26 13:36:00'), Timestamp('2025-03-26 13:36:00'), Timestamp('2025-03-26 13:36:00'), Timestamp('2025-03-26 13:36:00')]\n",
      "\n",
      "Step 2: Checking for duplicate timestamps\n",
      "Found 23818 duplicate timestamps\n",
      "Most common duplicate times (top 3):\n",
      "  2025-06-15 06:36:00: 36 occurrences\n",
      "  2025-06-12 08:42:00: 35 occurrences\n",
      "  2025-06-06 14:36:00: 24 occurrences\n",
      "Removed 23818 duplicate timestamps\n",
      "\n",
      "Step 3: Sorting by collect_time\n",
      "\n",
      "Step 4: Data validation and statistics\n",
      "Cleaned data shape: (3330, 6)\n",
      "Cleaned time range: 2025-03-26 13:36:00 to 2025-06-29 23:36:00\n",
      "Missing values per column:\n",
      "Most common time interval: 0 days 01:00:00\n",
      "Found 31 time gaps larger than 1 hour\n",
      "Largest gap: 0 days 07:00:00\n",
      "\n",
      "Data cleaning completed!\n",
      "Reduction: 23818 rows removed (87.73%)\n"
     ]
    }
   ],
   "source": [
    "for key in data.keys():\n",
    "    print(f\"Sensor ID: {key}, Station Name: {data[key]}\")\n",
    "    sensor_id = key\n",
    "    query = f\"\"\"\n",
    "    select * from deep_learning.hydro_data\n",
    "    where sensor_id = {sensor_id}\n",
    "    and collect_time > \"2024-07-01\" \n",
    "    and collect_time < \"2025-6-30\"\n",
    "    and uom =\"ft\"\n",
    "    order by collect_time;\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    df = clean_hydro_data(df)\n",
    "    df['seconds'] = df['collect_time'].astype('int64') // 10**9\n",
    "    df.to_parquet(f'data/{sensor_id}.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e566638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>collect_time</th>\n",
       "      <th>date_created</th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>uom</th>\n",
       "      <th>value</th>\n",
       "      <th>seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9247853</td>\n",
       "      <td>2025-03-26 13:36:00</td>\n",
       "      <td>2025-03-26 14:15:45.261</td>\n",
       "      <td>1413617665</td>\n",
       "      <td>ft</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1742996160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9248869</td>\n",
       "      <td>2025-03-26 14:36:00</td>\n",
       "      <td>2025-03-26 14:55:45.297</td>\n",
       "      <td>1413617665</td>\n",
       "      <td>ft</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>1742999760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9250266</td>\n",
       "      <td>2025-03-26 15:36:00</td>\n",
       "      <td>2025-03-26 15:50:45.240</td>\n",
       "      <td>1413617665</td>\n",
       "      <td>ft</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>1743003360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251155</td>\n",
       "      <td>2025-03-26 16:12:00</td>\n",
       "      <td>2025-03-26 16:25:45.286</td>\n",
       "      <td>1413617665</td>\n",
       "      <td>ft</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1743005520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9251790</td>\n",
       "      <td>2025-03-26 16:24:00</td>\n",
       "      <td>2025-03-26 16:50:45.294</td>\n",
       "      <td>1413617665</td>\n",
       "      <td>ft</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>1743006240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9252552</td>\n",
       "      <td>2025-03-26 17:06:00</td>\n",
       "      <td>2025-03-26 17:20:45.258</td>\n",
       "      <td>1413617665</td>\n",
       "      <td>ft</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1743008760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9252679</td>\n",
       "      <td>2025-03-26 17:12:00</td>\n",
       "      <td>2025-03-26 17:25:45.303</td>\n",
       "      <td>1413617665</td>\n",
       "      <td>ft</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1743009120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9253314</td>\n",
       "      <td>2025-03-26 17:30:00</td>\n",
       "      <td>2025-03-26 17:50:46.633</td>\n",
       "      <td>1413617665</td>\n",
       "      <td>ft</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>1743010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9253441</td>\n",
       "      <td>2025-03-26 17:42:00</td>\n",
       "      <td>2025-03-26 17:55:45.680</td>\n",
       "      <td>1413617665</td>\n",
       "      <td>ft</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1743010920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9254711</td>\n",
       "      <td>2025-03-26 18:30:00</td>\n",
       "      <td>2025-03-26 18:45:45.231</td>\n",
       "      <td>1413617665</td>\n",
       "      <td>ft</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1743013800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id        collect_time            date_created   sensor_id uom  value  \\\n",
       "0  9247853 2025-03-26 13:36:00 2025-03-26 14:15:45.261  1413617665  ft   0.14   \n",
       "1  9248869 2025-03-26 14:36:00 2025-03-26 14:55:45.297  1413617665  ft  -0.11   \n",
       "2  9250266 2025-03-26 15:36:00 2025-03-26 15:50:45.240  1413617665  ft  -0.31   \n",
       "3  9251155 2025-03-26 16:12:00 2025-03-26 16:25:45.286  1413617665  ft  -0.01   \n",
       "4  9251790 2025-03-26 16:24:00 2025-03-26 16:50:45.294  1413617665  ft  -0.29   \n",
       "5  9252552 2025-03-26 17:06:00 2025-03-26 17:20:45.258  1413617665  ft   0.00   \n",
       "6  9252679 2025-03-26 17:12:00 2025-03-26 17:25:45.303  1413617665  ft   0.00   \n",
       "7  9253314 2025-03-26 17:30:00 2025-03-26 17:50:46.633  1413617665  ft  -0.02   \n",
       "8  9253441 2025-03-26 17:42:00 2025-03-26 17:55:45.680  1413617665  ft   0.02   \n",
       "9  9254711 2025-03-26 18:30:00 2025-03-26 18:45:45.231  1413617665  ft   0.05   \n",
       "\n",
       "      seconds  \n",
       "0  1742996160  \n",
       "1  1742999760  \n",
       "2  1743003360  \n",
       "3  1743005520  \n",
       "4  1743006240  \n",
       "5  1743008760  \n",
       "6  1743009120  \n",
       "7  1743010200  \n",
       "8  1743010920  \n",
       "9  1743013800  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37b1d728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>collect_time</th>\n",
       "      <th>date_created</th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>uom</th>\n",
       "      <th>value</th>\n",
       "      <th>seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>204767</td>\n",
       "      <td>2024-07-02 00:06:00</td>\n",
       "      <td>2024-07-02 00:10:46.417</td>\n",
       "      <td>1836026194</td>\n",
       "      <td>ft</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1719878760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>204896</td>\n",
       "      <td>2024-07-02 00:12:00</td>\n",
       "      <td>2024-07-02 00:15:46.495</td>\n",
       "      <td>1836026194</td>\n",
       "      <td>ft</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>1719879120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>205151</td>\n",
       "      <td>2024-07-02 00:18:00</td>\n",
       "      <td>2024-07-02 00:25:46.419</td>\n",
       "      <td>1836026194</td>\n",
       "      <td>ft</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>1719879480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>205280</td>\n",
       "      <td>2024-07-02 00:24:00</td>\n",
       "      <td>2024-07-02 00:30:46.734</td>\n",
       "      <td>1836026194</td>\n",
       "      <td>ft</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>1719879840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>205409</td>\n",
       "      <td>2024-07-02 00:30:00</td>\n",
       "      <td>2024-07-02 00:35:46.671</td>\n",
       "      <td>1836026194</td>\n",
       "      <td>ft</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>1719880200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>240720</td>\n",
       "      <td>2024-07-02 23:30:00</td>\n",
       "      <td>2024-07-02 23:35:46.669</td>\n",
       "      <td>1836026194</td>\n",
       "      <td>ft</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1719963000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>240849</td>\n",
       "      <td>2024-07-02 23:36:00</td>\n",
       "      <td>2024-07-02 23:40:46.482</td>\n",
       "      <td>1836026194</td>\n",
       "      <td>ft</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1719963360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>240978</td>\n",
       "      <td>2024-07-02 23:42:00</td>\n",
       "      <td>2024-07-02 23:45:46.440</td>\n",
       "      <td>1836026194</td>\n",
       "      <td>ft</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1719963720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>241236</td>\n",
       "      <td>2024-07-02 23:48:00</td>\n",
       "      <td>2024-07-02 23:55:46.456</td>\n",
       "      <td>1836026194</td>\n",
       "      <td>ft</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1719964080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>241365</td>\n",
       "      <td>2024-07-02 23:54:00</td>\n",
       "      <td>2024-07-03 00:00:46.970</td>\n",
       "      <td>1836026194</td>\n",
       "      <td>ft</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1719964440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>237 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id        collect_time            date_created   sensor_id uom  \\\n",
       "240  204767 2024-07-02 00:06:00 2024-07-02 00:10:46.417  1836026194  ft   \n",
       "241  204896 2024-07-02 00:12:00 2024-07-02 00:15:46.495  1836026194  ft   \n",
       "242  205151 2024-07-02 00:18:00 2024-07-02 00:25:46.419  1836026194  ft   \n",
       "243  205280 2024-07-02 00:24:00 2024-07-02 00:30:46.734  1836026194  ft   \n",
       "244  205409 2024-07-02 00:30:00 2024-07-02 00:35:46.671  1836026194  ft   \n",
       "..      ...                 ...                     ...         ...  ..   \n",
       "472  240720 2024-07-02 23:30:00 2024-07-02 23:35:46.669  1836026194  ft   \n",
       "473  240849 2024-07-02 23:36:00 2024-07-02 23:40:46.482  1836026194  ft   \n",
       "474  240978 2024-07-02 23:42:00 2024-07-02 23:45:46.440  1836026194  ft   \n",
       "475  241236 2024-07-02 23:48:00 2024-07-02 23:55:46.456  1836026194  ft   \n",
       "476  241365 2024-07-02 23:54:00 2024-07-03 00:00:46.970  1836026194  ft   \n",
       "\n",
       "     value     seconds  \n",
       "240  -0.04  1719878760  \n",
       "241  -0.08  1719879120  \n",
       "242  -0.11  1719879480  \n",
       "243  -0.15  1719879840  \n",
       "244  -0.18  1719880200  \n",
       "..     ...         ...  \n",
       "472   0.81  1719963000  \n",
       "473   0.76  1719963360  \n",
       "474   0.71  1719963720  \n",
       "475   0.65  1719964080  \n",
       "476   0.61  1719964440  \n",
       "\n",
       "[237 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('data/1836026194.parquet')\n",
    "df = df[(df['collect_time'] > '2024-07-02') & (df['collect_time'] < '2024-07-03')]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62469785",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/hydro_data_1836026195.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05326aae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
